{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Notebook on how to run inference using `GPT-J`\n",
    "\n",
    "The GPT-J model was released in the [kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax) repository by Ben Wang and Aran Komatsuzaki. It is a GPT-2-like causal language model trained on the [Pile](https://pile.eleuther.ai/) dataset.\n",
    "\n",
    "This model was contributed by [Stella Biderman](https://huggingface.co/stellaathena).\n",
    "\n",
    "\n",
    "Documentation: [GPT-J](https://huggingface.co/docs/transformers/model_doc/gptj#gptj)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.12.3 torch==1.9.1 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "assert transformers.__version__ == \"4.12.3\", f\"wrong transformers version: {transformers.__version__}\"\n",
    "assert \"1.9.1\" in torch.__version__  , f\"wrong torch version: {torch.__version__}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the [fp16 branch](https://huggingface.co/EleutherAI/gpt-j-6B/tree/float16) which stores the fp16 weights, which could be used to further minimize the RAM usage. Combining all this it should take roughly 12.1GB of CPU RAM to load the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model and using the `generate` method\n",
    "\n",
    "Since we are using the `fp16` branch of the model it should fit on 16GB GPU for inference (P3) or (T4).\n",
    "loading the model fp16 branch (11.3GB) on `ec2` machine took 3 minutes and 32 seconds. Loading the model into memory took another 3 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d49d3cbf1e74e6d8eeb206148a1295a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/836 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9762f53daf7f47c998e0c69d0d6a8831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/11.3G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPTJForCausalLM\n",
    "import torch\n",
    "\n",
    "model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", revision=\"float16\", torch_dtype=torch.float16, low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In a shocking finding, scientists discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. They even have the same rights as us humans. The unicorns are only two foot tall--an easy target for a hunting rifle.\\n\\n\\nThe unicorns have the same rights as humans because they are, technically, human: they all stem from the same origin. They'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "\n",
    "device='cuda:0'\n",
    "model.to(device)\n",
    "\n",
    "prompt = \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \" \\\n",
    "         \"previously unexplored valley, in the Andes Mountains. Even more surprising to the \" \\\n",
    "         \"researchers was the fact that the unicorns spoke perfect English.\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "%timeit\n",
    "gen_tokens = model.generate(input_ids.to(device), do_sample=True, temperature=0.9, max_length=100,)\n",
    "\n",
    "tokenizer.batch_decode(gen_tokens)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the `gpt-j` from cache\n",
    "\n",
    "loading `gpt-j` from local cache took 3 minutes 16 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", revision=\"float16\", torch_dtype=torch.float16, low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model from `directory`\n",
    "\n",
    "loading the model from `disk` took 1m 23s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save_pretrained(\"./tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = GPTJForCausalLM.from_pretrained(\"tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `gpt-j` using `torch.load`\n",
    "\n",
    "loading the model with `torch.load` took 7.7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,GPTJForCausalLM\n",
    "import torch\n",
    "\n",
    "# load fp 16 model\n",
    "model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", revision=\"float16\", torch_dtype=torch.float16)\n",
    "torch.save(model, \"gptj.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"gptj.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "gen = pipeline(\"text-generation\",model=model,tokenizer=tokenizer,device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'My Name is philipp k. and I live just outside of Detroit. For most of my growing up years I knew that I wanted to be in the art world but had no idea where to start. I started taking art classes during high school and'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(\"My Name is philipp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating `model.tar.gz` for sagemaker deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "def compress(tar_dir=None,output_file=\"model.tar.gz\"):\n",
    "    with tarfile.open(output_file, \"w:gz\") as tar:\n",
    "        tar.add(tar_dir, arcname=os.path.sep)\n",
    "            \n",
    "\n",
    "import boto3\n",
    "\n",
    "def upload_file_to_s3(bucket_name=None,file_name=\"model.tar.gz\",key_prefix=\"\"):\n",
    "    s3 = boto3.resource('s3')\n",
    "    key_prefix_with_file_name = os.path.join(key_prefix,file_name)\n",
    "    s3.Bucket(bucket_name).upload_file(file_name, key_prefix_with_file_name)\n",
    "    return f\"s3://{bucket_name}/{key_prefix_with_file_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://hf-sagemaker-inference/gpt-j/model.tar.gz'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil \n",
    "import tarfile\n",
    "import torch\n",
    "from transformers import AutoTokenizer,GPTJForCausalLM\n",
    "\n",
    "def compress(tar_dir=None,output_file=\"model.tar.gz\"):\n",
    "    with tarfile.open(output_file, \"w:gz\") as tar:\n",
    "        tar.add(tar_dir, arcname=os.path.sep)\n",
    "            \n",
    "\n",
    "import boto3\n",
    "\n",
    "def upload_file_to_s3(bucket_name=None,file_name=\"model.tar.gz\",key_prefix=\"\"):\n",
    "    s3 = boto3.resource('s3')\n",
    "    key_prefix_with_file_name = os.path.join(key_prefix,file_name)\n",
    "    s3.Bucket(bucket_name).upload_file(file_name, key_prefix_with_file_name)\n",
    "    return f\"s3://{bucket_name}/{key_prefix_with_file_name}\"\n",
    "\n",
    "\n",
    "model_save_dir=\"./tmp\"\n",
    "bucket_name=\"hf-sagemaker-inference\"\n",
    "key_prefix=\"gpt-j\"\n",
    "src_inference_script= os.path.join(\"code\",\"inference.py\")\n",
    "dst_inference_script= os.path.join(model_save_dir,\"code\")\n",
    "\n",
    "os.makedirs(model_save_dir,exist_ok=True)\n",
    "os.makedirs(dst_inference_script,exist_ok=True)\n",
    "\n",
    "# load fp 16 model\n",
    "model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", revision=\"float16\", torch_dtype=torch.float16)\n",
    "torch.save(model, os.path.join(model_save_dir,\"gptj.pt\"))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "tokenizer.save_pretrained(model_save_dir)\n",
    "\n",
    "# copy inference script\n",
    "shutil.copy(src_inference_script, dst_inference_script)\n",
    "\n",
    "# create archive\n",
    "compress(model_save_dir)\n",
    "\n",
    "# upload to s3\n",
    "model_uri = upload_file_to_s3(bucket_name=bucket_name,key_prefix=key_prefix)\n",
    "model_uri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**bash scripting** -> after loading and saving model + copying `inference.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "tar zcvf model.tar.gz *\n",
    "aws s3 cp model.tar.gz s3://hf-sagemaker-inference/gpt-j/model.tar.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "os.environ[\"AWS_DEFAULT_REGION\"]=\"us-east-1\"\n",
    "\n",
    "\n",
    "iam_role=\"sagemaker_execution_role\"\n",
    "model_uri=\"s3://hf-sagemaker-inference/gpt-j/model.tar.gz\"\n",
    "\n",
    "iam_client = boto3.client('iam')\n",
    "role = iam_client.get_role(RoleName=iam_role)['Role']['Arn']\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "  model_data=model_uri,\n",
    "\ttransformers_version='4.12',\n",
    "\tpytorch_version='1.9',\n",
    "\tpy_version='py38',\n",
    "\trole=role, \n",
    ")\n",
    "\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1, # number of instances\n",
    "\tinstance_type='ml.g4dn.xlarge' #'ml.p3.2xlarge' # ec2 instance type\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Can you please let us know more details about your \\nexperiences with the bookkeeper.\\n\\nI received a call from Chris Foster requesting that you review the below \\nAgreement and return with any comments.  \\n\\nAs a'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict({\n",
    "\t'inputs': \"Can you please let us know more details about your \"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Can you please let us know more details about your xtraday, xtrading and portfolio strategies?\\nSo far, I have read that you have used a 15% drawdown when you exited the equity fund. Is this a safe'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict({\n",
    "\t'inputs': \"Can you please let us know more details about your \"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameterized request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Can you please let us know more details about your \\nissue?\\n\\nA:\\n\\nThe problem was caused by my lack of understanding on how web sockets \\n  worked. Once I understood how they work; I was able to fix'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict({\n",
    "\t'inputs': \"Can you please let us know more details about your \",\n",
    "  \"parameters\" : {\n",
    "    \"min_length\": 120,\n",
    "    \"temperature\": 0.9,\n",
    "  }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "custom end of sequence token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "\n",
    "end_sequence=\".\"\n",
    "temparature=40\n",
    "max_generated_token_length=50\n",
    "input=\"Can you please let us know more details about your \"\n",
    "\n",
    "predictor.predict({\n",
    "\t'inputs': input,\n",
    "  \"parameters\" : {\n",
    "    \"min_length\": int(len(input) + max_generated_token_length),\n",
    "    \"temperature\":temparature,\n",
    "    \"eos_token_id\": tokenizer.convert_tokens_to_ids(end_sequence)\n",
    "  }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
